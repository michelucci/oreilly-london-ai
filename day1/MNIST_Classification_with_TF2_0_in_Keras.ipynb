{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Classification with TF2.0 in Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelucci/oreilly-london-ai/blob/master/day1/MNIST_Classification_with_TF2_0_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-QOr6snaoyG",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Classification with TF2.0 in Keras\n",
        "\n",
        "(C) 2019 Umberto Michelucci\n",
        "\n",
        "umberto.michelucci@toelt.ai\n",
        "\n",
        "www.toelt.ai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITsLS3NLel8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "5a78f274-ac40-4f95-8560-4b790279eb7e"
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.16.5)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0rc3\n",
            "    Uninstalling tensorflow-1.15.0rc3:\n",
            "      Successfully uninstalled tensorflow-1.15.0rc3\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF3FFGnsay71",
        "colab_type": "code",
        "outputId": "954c368f-4580-4367-d887-0d310d7e52ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKjedvIrezmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5792829-c813-41e6-9420-7e6bd5a6049b"
      },
      "source": [
        "print (tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0DCQWSNa1GQ",
        "colab_type": "text"
      },
      "source": [
        "let's load the MNIST dataset first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzTqLY7La4pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets, info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)\n",
        "mnist_train, mnist_test = datasets['train'], datasets['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asM4dCLVi6FH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.0\n",
        "\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xmEsjYEjG-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1-WsG5sigL",
        "colab_type": "text"
      },
      "source": [
        "Now let's convert our data in a dataset, to make our training easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ljtmksmTiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_data = mnist_test.map(scale).batch(BATCH_SIZE)\n",
        "\n",
        "STEPS_PER_EPOCH = 50\n",
        "\n",
        "train_data = train_data.take(STEPS_PER_EPOCH)\n",
        "test_data = test_data.take(STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSI85WFDjNHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, label_batch = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLidKfPGkNZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69e66728-3b30-4bec-e10c-36479f866de7"
      },
      "source": [
        "print(image_batch.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraNKhrMjes0",
        "colab_type": "text"
      },
      "source": [
        "## First version with `compile()` and `fit()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v33qKC-cZRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "c6361576-def9-444d-8c0f-993506e114cb"
      },
      "source": [
        "mnist_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
        "                           kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "mnist_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "mnist_model.fit(train_data, epochs=NUM_EPOCHS)\n",
        "loss, acc = mnist_model.evaluate(test_data)\n",
        "\n",
        "print(\"Loss {}, Accuracy {}\".format(loss, acc))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "50/50 [==============================] - 6s 118ms/step - loss: 0.6178 - accuracy: 0.8341\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.2634 - accuracy: 0.9375\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1927 - accuracy: 0.9584\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 0.1571 - accuracy: 0.9688\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1200 - accuracy: 0.9772\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.6153 - accuracy: 0.9644\n",
            "Loss 0.6152513134479523, Accuracy 0.9643750190734863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Rjy7iYsnzc",
        "colab_type": "text"
      },
      "source": [
        "We can check the results for one single image easily with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHjspH-4huU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001) # 2.0 specific\n",
        "loss_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL2SyJsUk4xQ",
        "colab_type": "text"
      },
      "source": [
        "## Custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGVjE2rh1P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "metrics_names = mnist_model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Jc8D1YoE0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16a2c10d-64d4-4ef9-c8f6-04f5e9e6a980"
      },
      "source": [
        "print(metrics_names)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QuNr-30ojO_",
        "colab_type": "code",
        "outputId": "14104cee-4121-4e2e-e005-0cd458de4fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "  #Reset the metric accumulators\n",
        "  mnist_model.reset_metrics()\n",
        "\n",
        "  for image_batch, label_batch in train_data:\n",
        "    result = mnist_model.train_on_batch(image_batch, label_batch)\n",
        "    #print(\"train: \",\n",
        "    #      \"{}: {:.3f}\".format(metrics_names[0], result[0]),\n",
        "    #      \"{}: {:.3f}\".format(metrics_names[1], result[1]))\n",
        "  for image_batch, label_batch in test_data:\n",
        "    result = mnist_model.test_on_batch(image_batch, label_batch,\n",
        "                                 # return accumulated metrics\n",
        "                                 reset_metrics=False)\n",
        "  print(\"\\neval: \",\n",
        "        \"{}: {:.3f}\".format(metrics_names[0], result[0]),\n",
        "        \"{}: {:.3f}\".format(metrics_names[1], result[1]))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "eval:  loss: 0.195 accuracy: 0.964\n",
            "\n",
            "eval:  loss: 0.106 accuracy: 0.969\n",
            "\n",
            "eval:  loss: 0.089 accuracy: 0.962\n",
            "\n",
            "eval:  loss: 0.076 accuracy: 0.967\n",
            "\n",
            "eval:  loss: 0.071 accuracy: 0.965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EfpfVE7zgXl",
        "colab_type": "text"
      },
      "source": [
        "We can now recheck one single image to see if we perform better than before training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCT87uI_yK9_",
        "colab_type": "text"
      },
      "source": [
        "# MNIST with GPU acceleration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kB48r1hqnt",
        "colab_type": "code",
        "outputId": "084790b6-fa25-42e5-b835-e048215f44cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6TxlcyLiOTG",
        "colab_type": "code",
        "outputId": "4783937d-4407-4102-bd73-979ed516e045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found. Check the notebook settings.')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a4924a6763af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found. Check the notebook settings.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found. Check the notebook settings."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFFJylCviWOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tiipq2RfuKdH",
        "colab_type": "text"
      },
      "source": [
        "The following code have tensorflow operations that are placed on a GPU. At the moment the argmax has a bug and therefore has been placed on a CPU. A GPU make the code much faster than when using only a CPU (check above for the CPU only version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSz-0_EVhma7",
        "colab_type": "code",
        "outputId": "d0e81e13-5758-4f98-b312-4410d9106e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "for i in range(10): # Loop for the Epochs\n",
        "  print (\"\\nEpoch:\", i)\n",
        "  \n",
        "  for (batch, (images, labels)) in enumerate(dataset.take(60000)): # Loop for the mini-batches\n",
        "    if batch % 100 == 0:\n",
        "      print('.', end='')\n",
        "    labels = tf.cast(labels, dtype = tf.int64)\n",
        "    \n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      \n",
        "      with tf.device('/gpu:0'):    \n",
        "        logits = mnist_model(images, training=True)\n",
        "\n",
        "      with tf.device('/cpu:0'):\n",
        "        tgmax = tf.argmax(labels, axis = 1, output_type=tf.int64)\n",
        "        \n",
        "      with tf.device('/gpu:0'):  \n",
        "        loss_value = tf.losses.sparse_softmax_cross_entropy(tgmax, logits)\n",
        "\n",
        "        loss_history.append(loss_value.numpy())\n",
        "        grads = tape.gradient(loss_value, mnist_model.variables)\n",
        "        optimizer.apply_gradients(zip(grads, mnist_model.variables),\n",
        "                                    global_step=tf.train.get_or_create_global_step())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "..........\n",
            "Epoch: 1\n",
            "..........\n",
            "Epoch: 2\n",
            "..........\n",
            "Epoch: 3\n",
            "..........\n",
            "Epoch: 4\n",
            "..........\n",
            "Epoch: 5\n",
            "..........\n",
            "Epoch: 6\n",
            "..........\n",
            "Epoch: 7\n",
            "..........\n",
            "Epoch: 8\n",
            "..........\n",
            "Epoch: 9\n",
            "..........CPU times: user 1min 25s, sys: 3.41 s, total: 1min 28s\n",
            "Wall time: 1min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsp4lvlKl9Oc",
        "colab_type": "text"
      },
      "source": [
        "## Custom training loops with gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmso3U35mBQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "fa26ad29-b7ef-48e2-a30b-3cdbe2be7d6b"
      },
      "source": [
        "# Create the metrics\n",
        "loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "mnist_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
        "                           kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = mnist_model(inputs, training=True)\n",
        "    regularization_loss = tf.math.add_n(mnist_model.losses)\n",
        "    pred_loss = loss_fn(labels, predictions)\n",
        "    total_loss = pred_loss + regularization_loss\n",
        "\n",
        "  gradients = tape.gradient(total_loss, mnist_model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, mnist_model.trainable_variables))\n",
        "\n",
        "  # Update the metrics\n",
        "  loss_metric.update_state(total_loss)\n",
        "  accuracy_metric.update_state(labels, predictions)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_metric.reset_states()\n",
        "  accuracy_metric.reset_states()\n",
        "\n",
        "  for inputs, labels in train_data:\n",
        "    train_step(inputs, labels)\n",
        "  \n",
        "  mean_loss = loss_metric.result()\n",
        "  mean_accuracy = accuracy_metric.result()\n",
        "\n",
        "  print('Epoch: ', epoch)\n",
        "  print('  loss:     {:.3f}'.format(mean_loss))\n",
        "  print('  accuracy: {:.3f}'.format(mean_accuracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "  loss:     0.622\n",
            "  accuracy: 0.831\n",
            "Epoch:  1\n",
            "  loss:     0.266\n",
            "  accuracy: 0.943\n",
            "Epoch:  2\n",
            "  loss:     0.198\n",
            "  accuracy: 0.957\n",
            "Epoch:  3\n",
            "  loss:     0.151\n",
            "  accuracy: 0.973\n",
            "Epoch:  4\n",
            "  loss:     0.125\n",
            "  accuracy: 0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghus5gOGmUOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}